{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565285bf",
   "metadata": {},
   "source": [
    "<div class='bar_title'></div>\n",
    "\n",
    "*Enterprise AI*\n",
    "\n",
    "# Tutorial 6 - Code Modulerization and Experimental Tracking\n",
    "\n",
    "Gunther Gust / Viet Nguyen<br>\n",
    "Chair of Enterprise AI\n",
    "\n",
    "Summer Semester 25\n",
    "\n",
    "<img src=\"https://github.com/GuntherGust/tds2_data/blob/main/images/d3.png?raw=true\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec10b42",
   "metadata": {},
   "source": [
    "In previous assignments, you explored how to use Jupyter notebooks to quickly prototype simple machine learning projects. Notebooks are designed for rapid experimentation, allowing you to combine code, results, visualizations, and Markdown-based documentation in a single, shareable environment. This format is especially effective for teaching and collaboration (arguarbly, some hardcore programmers think otherwise $^{[1]}$).\n",
    "\n",
    "Despite their widespread adoption in the scientific and data science communities, Jupyter notebooks have several limitations when it comes to modern software development. If you inspect a notebook as a plain text file, you'll notice that it stores all content (code, outputs, images, and Markdown texts) in a single JSON object. This design introduces several drawbacks:\n",
    "\n",
    "1. *Version control is difficult*: Every time a notebook is executed, the outputs change, even if the code doesn’t. This makes it hard to track meaningful changes and collaborate effectively in an iterative development process.\n",
    "2. *Non-linear execution*: Code in notebooks can be run in any order, which breaks the logical, linear flow that most programming languages rely on. This can lead to hidden state issues and non-reproducible behavior.\n",
    "3. *Poor modularity and code reuse*: Since notebooks often contain all logic in a single document, it's harder to separate concerns, build reusable components, or maintain clean architecture.\n",
    "4. *Limited support for testing and deployment*: Notebooks lack native mechanisms for automated testing, continuous integration, or production deployment workflows.\n",
    "\n",
    "The last two issues are especially problematic, which maked notebooks unsuitable for building production-level software systems.\n",
    "\n",
    "In this tutorial, we will walk through the next step toward production-level practices by modularizing our notebook code into separate Python files (`.py`). This means that we will extract the core logic such as data processing, model training, and model evaluation into separate modules that can be `imported` and reused. This module structure not only makes the codebase easier to maintain and test, but also helps to integrate our code with tools like [Weights & Biases](https://wandb.ai/) for experiment tracking. By isolating components, we gain flexibility to rerun or update parts of the pipeline without touching the entire notebook. This structure is essential for deploying models in real-world systems, where reproducibility, versioning, and automation are critical.\n",
    "\n",
    "---\n",
    "$^{[1]}$: In 2018, there was a debate on Twitter between Joel Grus and Jeremy Howard regarding the merits of Jupyter notebooks. Joel Grus voiced his criticisms in a talk titled [_I Don’t Like Notebooks_](https://youtu.be/7jiPeIFXb6U?feature=shared), arguing that notebooks encourage poor software engineering practices. Two years later, Jeremy Howard offered a rebuttal in his talk [_I Like Notebooks_](https://youtu.be/9Q6sLbz37gk?feature=shared) to advocate how notebooks have been instrumental to the success of his `fastai` project and to accessible deep learning education.\n",
    "\n",
    "Both Grus and Howard are respected figures in the data science community, though they appeal to different audiences. Joel Grus is known for his emphasis on foundational understanding and software discipline, while Jeremy Howard is recognized for democratizing deep learning through practical, code-first education at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40b6796",
   "metadata": {},
   "source": [
    "## 1. Modularization of the notebook\n",
    "\n",
    "In this tutorial, we will work with a modularized version of the code located in the `tutorial_06_python_code` folder. The structure of the project is organized as follows:\n",
    "\n",
    "```\n",
    "tutorial_06_python_code/\n",
    "│\n",
    "├── data/                              # data folder\n",
    "│   └── housing.csv                    # dataset\n",
    "│\n",
    "├── src/                               # Source code directory\n",
    "│   ├── __init__.py                    # Making the folder into a Python package\n",
    "│   ├── data_loader.py                 # Functions to load and inspect data\n",
    "│   ├── preprocessing.py               # Feature engineering methods (e.g., encoding, imputing missing values)\n",
    "│   ├── model.py                       # Model definition, training, and evaluation\n",
    "│   ├── config.py                      # Constants like test size, random state, etc.\n",
    "│   └── utils.py                       # Helper functions (e.g., for visualization, metrics)\n",
    "│\n",
    "└── run.py                             # Entry point script to execute the training pipeline\n",
    "```\n",
    "\n",
    "This structure allows us to reuse some of the logic developed in Tutorial 2, while improving clarity and maintainability. Rather than placing all code inside a single notebook, we break the workflow into separate components:\n",
    "- `data/`: contains the datasets to be experimented. \n",
    "- `src/`: contains the core logic for each step of the machine learning workflow, including data loading, preprocessing, modeling, and supporting utilities.\n",
    "- `run.py`: serves as the entry point to the pipeline. It integrates all components from the `src directory` into a coherent and executable training process.\n",
    "\n",
    "Note that we simplify the modularization by not putting the implementations into separate `classes`, but only `functions`. You can read more about modularization of a machine learning project in chapter 10 of the online book [The Pragmatic Programmer for Machine Learning](https://ppml.dev/) by Marco Scutari, Mauro Malvestio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfecf595",
   "metadata": {},
   "source": [
    "### 1.1. What is `__init__.py`?\n",
    "It is a special file that turns a directory into a **Python package**. Its presence tells Python to treat this folder as a package so its modules can be imported. For example, this allows you to perform `relative importing`. Suppose you want to import a function `load_data` from `data_loader.py` to `model.py`:\n",
    "\n",
    "```python\n",
    "# In model.py\n",
    "from .data_loader import load_data\n",
    "```\n",
    "\n",
    "Without `__init__.py`:\n",
    "- Relative imports break because `src` isn't a recognized package\n",
    "- Tools like `pytest` or `pip` might throw `ModuleNotFoundError` \n",
    "- Harder to transition your code into a proper library or deployment pipeline\n",
    "\n",
    "To avoid unwanted bugs and make your code structure behave consistently in all contexts, it is recommended to add an empty `__init__.py` to `src` folder, and all sub-folders of `src`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25021a",
   "metadata": {},
   "source": [
    "### 1.2. Modularized Code\n",
    "\n",
    "If you're working on this notebook locally, you'll notice that the code logic has now been modularized into separate components inside the `src` folder, following the project structure described earlier. In `run.py`, we bring these components together to form a complete training pipeline using the following imports:\n",
    "\n",
    "```python\n",
    "from src.config import TEST_SIZE, DATA_PATH, DATA_RANDOM_STATE, MODEL_RANDOM_STATE\n",
    "from src.data_loader import load_housing_data\n",
    "from src.preprocessing import split_data, impute_train_data, impute_test_data, remove_categorical_columns\n",
    "from src.model import create_forest_regressor, train_model, get_predictions\n",
    "from src.utils import compute_mae, compute_mape\n",
    "```\n",
    "\n",
    "Note that this modularization isn't meant to be production-ready or fully optimized. Its purpose is to demonstrate how separating different responsibilities (like configuration, preprocessing, modeling, and utilities) makes your code cleaner, more maintainable, and reusable.\n",
    "\n",
    "For example, the function `impute_train_data` is written in a way that allows reuse with different imputation strategies. In the current code, we apply it with the `mean` strategy to impute numerical columns, while removing categorical columns for simplicity. However, if you decide to handle categorical features, you can reuse the same function like this:\n",
    "\n",
    "```python\n",
    "cat_imputer, cat_train = impute_train_data(X_test, [\"furnishingstatus\"], \"most_frequent\")\n",
    "```\n",
    "Similarly, the utility functions `compute_mae` and `compute_mape` are designed to be generic. They can be reused across different datasets as long as the inputs follow the expected format described in each function’s docstring. Modular design like this is especially helpful when scaling up to more complex projects or transitioning toward deployment, where testability, maintainability, and reusability are essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c9151c",
   "metadata": {},
   "source": [
    "## 1.3. Running the code\n",
    "\n",
    "You can run the code by simply go into the terminal, and type the below command at the project folder:\n",
    "\n",
    "> python run.py\n",
    "\n",
    "You will see this input at the terminal:\n",
    "\n",
    "![Running code in terminal](images/code_terminal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e880d7",
   "metadata": {},
   "source": [
    "If you are not familiar with using terminals, we can also run the code from this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02664ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample: Mean Absolute Error: 414244.20\t Mean Absolute Percentage Error: 0.09\n",
      "Out-sample: Mean Absolute Error: 1045208.27\t Mean Absolute Percentage Error: 0.24\n"
     ]
    }
   ],
   "source": [
    "!python ./tutorial_06_python_code/run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8610f2d",
   "metadata": {},
   "source": [
    "When you add `!` in a code cell in Jupyter notebook, it calls the system shell (terminal) behind the scenes, similar to running it directly at the terminal. Here, since your notebook is outside of the folder `tutorial_06_python_code`, we have to specify the path to where the `run.py` file exists. The `./` is a relative path that tells Python to look for the file starting from the *current directory*. Here, it means we start looking at the `course-materials`, and go one level into the `tutorial_06_python_code` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7069ba73",
   "metadata": {},
   "source": [
    "## 1.4. Import functions to notebook\n",
    "\n",
    "In addition to running the `run.py` script, you can import and reuse the functions defined in the `src` folder inside a Jupyter notebook. For example, let's re-use the load_data function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc70ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_06_python_code.src.data_loader import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc39fc",
   "metadata": {},
   "source": [
    "Instead of using the `housing.csv` data, we can load the `insurance.csv` inside the `course-materials/solutions` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "479f1c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./solutions/data/insurance.csv\"\n",
    "df = load_data(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2011311",
   "metadata": {},
   "source": [
    "Then, we can re-use the imputation function to impute the categorical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6e4c0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "bmi         4\n",
       "children    0\n",
       "smoker      0\n",
       "region      7\n",
       "charges     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40793ccf",
   "metadata": {},
   "source": [
    "Let's assume our entire dataset is a train set, we can reuse the `impute_train_data` function to impute the categorical `region` column using `most_frequent` strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c089f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "bmi         4\n",
       "children    0\n",
       "smoker      0\n",
       "region      0\n",
       "charges     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tutorial_06_python_code.src.preprocessing import impute_train_data\n",
    "imputer, cat_imputed = impute_train_data(df, [\"region\"], \"most_frequent\")\n",
    "df[\"region\"] = cat_imputed\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0edf7c5",
   "metadata": {},
   "source": [
    "This short demo shows how modularizing your code not only makes your project easier to maintain, but also allows you to reuse specific functions directly in a notebook for experimentation or analysis. Whether you're running the full pipeline via `run.py` or interacting in Jupyter, separating logic into modules gives you flexibility and clarity. This is a key habit for scaling up to real-world projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ae7b2",
   "metadata": {},
   "source": [
    "## 2. Experimental Tracking with Weight & Biases (WanDB)\n",
    "\n",
    "Weights & Biases (WanDB) is a powerful tool that helps data scientists track machine learning experiments, datasets, and system information with just a few lines of code. WanDB is free to use for individuals and research organizations, which makes it widely accessible for academic and non-commercial projects. It supports awide range of machine learning frameworks without requiring users to switch tools. These include TensorFlow, Keras, PyTorch, Scikit-learn, FastAI, and more.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/wandb.webp\" alt=\"WanDB\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "### Key features\n",
    "All tracked information is sent to an intuitive inferface (UI) provided by Weights & Biases. This dashboard allows for:\n",
    "- Easy visualization and analysis of metrics and logs\n",
    "- Fast and interactive model comparison, including hyperparameter tuning results and experiment histories\n",
    "- Collaboration support: you can share results with your team via the platform's web interface or through custom reports.\n",
    "\n",
    "Weights & Biases is a highly flexible and scalable tool used by individual developers and large AI research teams (e.g., OpenAI, DeepMind, and NVIDIA) to track experiments, visualize results, optimize models, and ensure reproducibility in machine learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60bdd60",
   "metadata": {},
   "source": [
    "### 2.1. Why is WanDB important in large-scale Machine Learning?\n",
    "\n",
    "Deep learning models often involve training large neural networks on massive datasets, which can take hours, days, or even weeks to complete. During this long process, many experiments are run to try different architectures, hyperparameters, or training strategies to improve performance. Keeping track of all these experiments manually is nearly impossible.\n",
    "\n",
    "Weights & Biases (WandB) is a powerful tool that helps data scientists and researchers to automatically track, visualize, and compare these experiments in real-time. It enables easy monitoring of training progress, recording of metrics like loss and accuracy, saving model checkpoints, and sharing results with collaborators - all in one centralized platform. This makes managing complex deep learning workflows much more organized and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfe266c",
   "metadata": {},
   "source": [
    "### 2.2. Getting started with WanDB:\n",
    "\n",
    "Before using WanDB, we need to set up a WanDB account and install the library altogether:\n",
    "\n",
    "1. Create an account: visit [WanDB's website](https://wandb.ai/) to register an account with your email\n",
    "2. Install wandb (for local users): `pip install wandb`. If you use VS CodeSpace, skip this.\n",
    "3. After logging in, select your profile at the top-right window of the dashboard, then select `API key` and copy the key\n",
    "4. Initialize a wandb project using `wandb.init()`, which will prompt you to input your API Key\n",
    "\n",
    "We will do the 4) step in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6aa87d",
   "metadata": {},
   "source": [
    "### 2.2. Tracking experiment with WanDB\n",
    "\n",
    "In this section, we will explore another dataset stored in `tutorial_06_python_code/data/advertising.csv`, build a machine learning model, and use WanDB to track the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f475740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  Radio  Newspaper  Sales\n",
       "0  230.1   37.8       69.2   22.1\n",
       "1   44.5   39.3       45.1   10.4\n",
       "2   17.2   45.9       69.3    9.3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv = load_data(\"./tutorial_06_python_code/data/advertising.csv\")\n",
    "adv.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611600aa",
   "metadata": {},
   "source": [
    "Let's initialize a WanDB project. If you run the code below the first time, it will asks you to input the API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfa0633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maveragejett\u001b[0m (\u001b[33mdmt-linkpred\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/course-materials/wandb/run-20250608_184901-660075hg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmt-linkpred/demo-sale-regression/runs/660075hg' target=\"_blank\">electric-frog-3</a></strong> to <a href='https://wandb.ai/dmt-linkpred/demo-sale-regression' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmt-linkpred/demo-sale-regression' target=\"_blank\">https://wandb.ai/dmt-linkpred/demo-sale-regression</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmt-linkpred/demo-sale-regression/runs/660075hg' target=\"_blank\">https://wandb.ai/dmt-linkpred/demo-sale-regression/runs/660075hg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dmt-linkpred/demo-sale-regression/runs/660075hg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x79745a6d58a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"demo-sale-regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b86a67",
   "metadata": {},
   "source": [
    "Next, we create a pipeline as follows:\n",
    "- Split the dataset\n",
    "- Create a ML model\n",
    "- Fit the train set\n",
    "- Evaluate on the test set using MSE\n",
    "- Log the loss function and test results on WanDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d812a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = adv.drop(\"Sales\", axis=1)\n",
    "Y = adv[\"Sales\"]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "\n",
    "model = AdaBoostRegressor(random_state=0)\n",
    "# train\n",
    "model.fit(X_train, Y_train)\n",
    "# log training loss for each boosting iteration using staged_predict\n",
    "for i, y_pred_train in enumerate(model.staged_predict(X_train)):\n",
    "    train_loss = mean_squared_error(Y_train, y_pred_train)\n",
    "    wandb.log({\"train_mse\": train_loss, \"boosting_iteration\": i})\n",
    "\n",
    "# test predictions\n",
    "preds = model.predict(X_test)\n",
    "mse = mean_squared_error(preds, Y_test)\n",
    "wandb.log({\"test_mse\": mse})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ca671",
   "metadata": {},
   "source": [
    "If you have finished running the experiments, run `wandb.finish()` to save all the artifacts to WanDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b38974c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>boosting_iteration</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test_mse</td><td>▁</td></tr><tr><td>train_mse</td><td>█▆▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>boosting_iteration</td><td>49</td></tr><tr><td>test_mse</td><td>1.46527</td></tr><tr><td>train_mse</td><td>0.6175</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">electric-frog-3</strong> at: <a href='https://wandb.ai/dmt-linkpred/demo-sale-regression/runs/660075hg' target=\"_blank\">https://wandb.ai/dmt-linkpred/demo-sale-regression/runs/660075hg</a><br> View project at: <a href='https://wandb.ai/dmt-linkpred/demo-sale-regression' target=\"_blank\">https://wandb.ai/dmt-linkpred/demo-sale-regression</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250608_184901-660075hg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbfc19b",
   "metadata": {},
   "source": [
    "You can go to the Dashboard on WanDB's Dashboard to see the visualizations of the above run:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/exp.png\" alt=\"WanDB\" width=\"1000\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4461a6",
   "metadata": {},
   "source": [
    "Here is a simple experiment. When you want to explore which model performs best among several choices, or when tuning different configurations of the same model, the number of experiments can quickly grow out of control. It is critical to keep track of all experiments carefully. This means recording not only the final results but also the settings used (like hyperparameters, data splits, and random seeds), and intermediate progress (such as loss curves). This careful tracking allows you to:\n",
    "\n",
    "- Easily compare models and configurations to identify the best performing one without guessing or manual record keeping.\n",
    "- Ensure reproducibility, meaning that anyone (including future you!) can rerun an experiment and get the same results by following the saved configuration and data.\n",
    "- Debug and recover from interruptions by resuming training from saved checkpoints instead of starting over.\n",
    "- Share your work efficiently with teammates, who can then understand, reproduce, and build upon your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ecb07e",
   "metadata": {},
   "source": [
    "## 2.3. WanDB in Research\n",
    "\n",
    "For example, in one of our research projects, a model with more than 10 different configurations was run multiple times. Since the dataset and model size are huge, the whole process took nearly a month to complete all experiments! Below are 40 runs that took about a week to complete:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/wandb_research.png\" alt=\"WanDB\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "Using WanDB can help us to:\n",
    "- Track training progress by monitoring loss and other metrics over time, ensuring the model trains properly.\n",
    "- Recover training in case of crashes or interruptions by loading checkpoints instead of retraining from scratch (saving weeks of compute time).\n",
    "- Organize experiments and easily identify the best hyperparameter combinations without manual notes or spreadsheet chaos.\n",
    "- Visualize results with intuitive dashboards and share findings instantly with our collaborators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf36ed",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you've learned the importance of modularizing your code by moving from a monolithic Jupyter notebook to a more organized, script-based workflow. This is a crucial step in building scalable and maintainable machine learning projects. You also explored how to use Weights & Biases (WandB) to track experiments, monitor model performance, and ensure reproducibility, which are all essential practices for real-world ML development. \n",
    "\n",
    "In the next assignment, you'll take this one step further by integrating WandB with ZenML. With ZenML, you'll learn how to structure your workflows into reusable pipeline steps (such as data loading, training, evaluation, and deployment), while WandB will continue to serve as a tracking and visualization layer for these steps. Together, they provide a powerful foundation for building reproducible, automated, and collaborative ML systems from experimentation to deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
