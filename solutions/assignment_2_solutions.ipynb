{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c36e3fc",
   "metadata": {},
   "source": [
    "<div class=\"bar_title\"></div>\n",
    "\n",
    "*Enterprise AI*\n",
    "\n",
    "# Assignment 2 - Machine Learning Pipeline with ZenML\n",
    "\n",
    "Gunther Gust / Viet Nguyen<br>\n",
    "Chair of Enterprise AI\n",
    "\n",
    "Summer Semester 25\n",
    "\n",
    "<img src=\"https://github.com/GuntherGust/tds2_data/blob/main/images/d3.png?raw=true\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e6fe8",
   "metadata": {},
   "source": [
    "In this assignment, your goal is to modularize each part of the machine learning process using ZenML `step` and `pipeline`:\n",
    "- Load, split and preprocess the data\n",
    "- Train a model\n",
    "- Evaluate a model\n",
    "\n",
    "Please DO NOT remove or modify the cells with `assert` functions. They are meant to let you know that your functions are working correctly, and you are on the right track. In addition, you PASS the assignment ONLY IF **your code logic is correct** AND **you pass all the `assert` functions**. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d215b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/zenml/cli/utils.py:43: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Initializing ZenML repository at /workspaces/assignment-2-solution.\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Initializing ZenML repository at /workspaces/assignment-2-solution.\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /workspaces/assignment-2-solution.\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Initializing ZenML repository at /workspaces/assignment-2-solution.\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Initializing ZenML repository at /workspaces/assignment-2-solution.\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Initializing ZenML repository at /workspaces/assignment-2-solution.\n",
      "\u001b[1;35mSetting the repo active project to 'default'.\u001b[0m\n",
      "\u001b[33mSetting the repo active stack to default.\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mZenML repository initialized at \u001b[0m\u001b[2;35m/workspaces/\u001b[0m\u001b[2;95massignment-2-solution.\u001b[0m\n",
      "\u001b[2;32m⠦\u001b[0m\u001b[2;36m Initializing ZenML repository at /workspaces/assignment-2-solution.\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Initializing ZenML repository at /workspaces/assignment-2-solution.\n",
      "\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mThe local active stack was initialized to \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m. This local configuration \u001b[0m\n",
      "\u001b[2;36mwill only take effect when you're running ZenML from the initialized repository \u001b[0m\n",
      "\u001b[2;36mroot, or from a subdirectory. For more information on repositories and \u001b[0m\n",
      "\u001b[2;36mconfigurations, please visit \u001b[0m\n",
      "\u001b[2;4;94mhttps://docs.zenml.io/user-guides/production-guide/understand-stacks.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# run this cell to initialize a fresh zenml project\n",
    "!rm -rf .zen\n",
    "!zenml init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a43b8",
   "metadata": {},
   "source": [
    "## 1. Data Inspection\n",
    "\n",
    "The dataset contains daily weather observations of Perth, Australia. Each row represents the weather conditions for a given day. Our task is to predict whether it will rain tomorrow based on today's weather conditions (features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7e9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f47d2",
   "metadata": {},
   "source": [
    "(a) Load the dataset \"weather.csv\" in the `data` folder. Display the first 7 rows of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e6cc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>WindDir3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-07-01</td>\n",
       "      <td>2.7</td>\n",
       "      <td>18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>9.1</td>\n",
       "      <td>ENE</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1027.6</td>\n",
       "      <td>1024.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>18.1</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-07-02</td>\n",
       "      <td>6.4</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NE</td>\n",
       "      <td>22.0</td>\n",
       "      <td>ESE</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1024.1</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>19.7</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-07-03</td>\n",
       "      <td>6.5</td>\n",
       "      <td>19.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>7.3</td>\n",
       "      <td>NE</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1016.8</td>\n",
       "      <td>1015.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>17.7</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-07-04</td>\n",
       "      <td>9.5</td>\n",
       "      <td>19.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>W</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NNE</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1019.3</td>\n",
       "      <td>1018.4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>17.7</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-07-05</td>\n",
       "      <td>9.5</td>\n",
       "      <td>16.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>4.9</td>\n",
       "      <td>WSW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>SW</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1020.4</td>\n",
       "      <td>1022.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008-07-06</td>\n",
       "      <td>0.7</td>\n",
       "      <td>15.9</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>9.3</td>\n",
       "      <td>NNE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>NE</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>1029.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>15.5</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008-07-07</td>\n",
       "      <td>0.7</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>9.3</td>\n",
       "      <td>N</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NE</td>\n",
       "      <td>NNE</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1028.9</td>\n",
       "      <td>1024.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>17.9</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine WindGustDir  \\\n",
       "0  2008-07-01      2.7     18.8       0.0          0.8       9.1         ENE   \n",
       "1  2008-07-02      6.4     20.7       0.0          1.8       7.0          NE   \n",
       "2  2008-07-03      6.5     19.9       0.4          2.2       7.3          NE   \n",
       "3  2008-07-04      9.5     19.2       1.8          1.2       4.7           W   \n",
       "4  2008-07-05      9.5     16.4       1.8          1.4       4.9         WSW   \n",
       "5  2008-07-06      0.7     15.9       6.8          2.4       9.3         NNE   \n",
       "6  2008-07-07      0.7     18.3       0.0          0.8       9.3           N   \n",
       "\n",
       "   WindGustSpeed WindDir9am WindDir3pm  ...  WindSpeed3pm  Humidity9am  \\\n",
       "0           20.0        NaN          E  ...           7.0         97.0   \n",
       "1           22.0        ESE        ENE  ...           9.0         80.0   \n",
       "2           31.0        NaN        WNW  ...           4.0         84.0   \n",
       "3           26.0        NNE        NNW  ...           6.0         93.0   \n",
       "4           44.0          W         SW  ...          17.0         69.0   \n",
       "5           24.0        ENE         NE  ...           7.0         86.0   \n",
       "6           37.0         NE        NNE  ...          13.0         72.0   \n",
       "\n",
       "   Humidity3pm  Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  \\\n",
       "0         53.0       1027.6       1024.5       2.0       3.0      8.5   \n",
       "1         39.0       1024.1       1019.0       0.0       6.0     11.1   \n",
       "2         71.0       1016.8       1015.6       1.0       3.0     12.1   \n",
       "3         73.0       1019.3       1018.4       6.0       6.0     13.2   \n",
       "4         57.0       1020.4       1022.1       7.0       5.0     15.9   \n",
       "5         41.0       1032.0       1029.6       0.0       1.0      6.9   \n",
       "6         36.0       1028.9       1024.2       1.0       5.0      8.7   \n",
       "\n",
       "   Temp3pm  RainTomorrow  \n",
       "0     18.1            No  \n",
       "1     19.7            No  \n",
       "2     17.7           Yes  \n",
       "3     17.7           Yes  \n",
       "4     16.0           Yes  \n",
       "5     15.5            No  \n",
       "6     17.9            No  \n",
       "\n",
       "[7 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/weather.csv\")\n",
    "data.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4231fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "assert data.shape == (3193, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c3105",
   "metadata": {},
   "source": [
    "(b) Summarize the missing values of individual columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6bbad7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date               0\n",
       "MinTemp            0\n",
       "MaxTemp            1\n",
       "Rainfall           0\n",
       "Evaporation        1\n",
       "Sunshine           5\n",
       "WindGustDir        5\n",
       "WindGustSpeed      5\n",
       "WindDir9am       134\n",
       "WindDir3pm         7\n",
       "WindSpeed9am       0\n",
       "WindSpeed3pm       1\n",
       "Humidity9am        9\n",
       "Humidity3pm        8\n",
       "Pressure9am        1\n",
       "Pressure3pm        1\n",
       "Cloud9am           2\n",
       "Cloud3pm           4\n",
       "Temp9am            0\n",
       "Temp3pm            1\n",
       "RainTomorrow       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde39086",
   "metadata": {},
   "source": [
    "(c) Which column has the most missing values? Answer in one line:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec754d",
   "metadata": {},
   "source": [
    "WindDir9am"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b4895",
   "metadata": {},
   "source": [
    "## 2. ML Pipeline with ZenML\n",
    "\n",
    "Now that we have a good understanding of our data, we can begin building our pipeline using ZenML. A ZenML pipeline consists of a series of modular **steps**, each representing a distinct stage in the machine learning workflow such as data loading, feature engineering, or model tuning. These steps are defined as functions that comply with the ZenML framework's specifications. Once all the individual steps are implemented, they can be assembled into a single Python function that defines the complete **pipeline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b665ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "from zenml import pipeline, step\n",
    "from typing_extensions import Annotated\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada174c",
   "metadata": {},
   "source": [
    "#### 2.1. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df23e8",
   "metadata": {},
   "source": [
    "(a) Load the data from the csv file. Please use `pd.read_csv()` with parameter `index_col` as the `Date` column of the dataset. \n",
    "\n",
    "The `:param name:` is a description of the expected behavior of the parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cf2eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(enable_cache=False)\n",
    "def loading_data(filename: str) -> Annotated[pd.DataFrame, \"input_data\"]:\n",
    "    \"\"\" \n",
    "    Loads a CV File and transforms it to a Pandas DataFrame\n",
    "    :param filename: the file name (including the path) of the dataset\n",
    "    \n",
    "    return pandas DataFrame of the dataset\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filename, index_col=\"Date\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1650a961",
   "metadata": {},
   "source": [
    "(b) Split the data set into train/test ratio of 7/3. Use `random_state=0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0dc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def split_data(dataset:pd.DataFrame, label: str) -> Tuple[\n",
    "    Annotated[pd.DataFrame, \"X_train\"],\n",
    "    Annotated[pd.DataFrame, \"X_test\"],\n",
    "    Annotated[pd.Series, \"y_train\"],\n",
    "    Annotated[pd.Series, \"y_test\"]]:\n",
    "    \"\"\"\n",
    "    Splits a dataset into training and testing sets.\n",
    "    :param dataset: the pandas DataFrame loaded from the csv file\n",
    "    :param label: the column of target variable. Example usage: y = dataset[label] will get the values of target variable\n",
    "\n",
    "    return X_train, X_test, y_train, y_test of the dataset\n",
    "    \"\"\"\n",
    "    X = dataset.drop(label, axis=1)\n",
    "    Y = dataset[label]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, shuffle=False)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16db61f",
   "metadata": {},
   "source": [
    "(c) As observed earlier, our dataset contains missing values. To address this, we will define a pipeline step that handles imputation. Specifically, numerical features will be imputed using the `median strategy`, while categorical features will use the `most frequent value`. These imputation strategies should be applied consistently to both the training and test datasets. The step function will return the transformed train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e801d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def impute_missing_values(X_train:pd.DataFrame, X_test:pd.DataFrame) -> Tuple[Annotated[pd.DataFrame, \"X_train_imputed\"],Annotated[pd.DataFrame, \"X_test_imputed\"]]:\n",
    "    \"\"\"\n",
    "    Imputes missing values in training and testing datasets.\n",
    "    :param X_train: feature columns of the train set\n",
    "    :param X_test: feature columns of the test set\n",
    "\n",
    "    return train and test sets that have been properly imputed\n",
    "    \"\"\"\n",
    "    categorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "    numerical_imputer = SimpleImputer(strategy=\"median\")\n",
    "    categorical_columns = X_train.select_dtypes(include=\"object\").columns\n",
    "    numerical_columns = X_train.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "    X_train[numerical_columns] = pd.DataFrame(\n",
    "    numerical_imputer.fit_transform(X_train[numerical_columns]),index=X_train.index, columns=numerical_columns\n",
    "    )\n",
    "\n",
    "    X_test[numerical_columns] = pd.DataFrame(\n",
    "    numerical_imputer.transform(X_test[numerical_columns]),index=X_test.index, columns=numerical_columns\n",
    "    )\n",
    "\n",
    "    X_train[categorical_columns] = pd.DataFrame(\n",
    "    categorical_imputer.fit_transform(X_train[categorical_columns]),index=X_train.index, columns=categorical_columns\n",
    "    )\n",
    "\n",
    "    X_test[categorical_columns] = pd.DataFrame(\n",
    "    categorical_imputer.transform(X_test[categorical_columns]),index=X_test.index, columns=categorical_columns\n",
    "    )\n",
    "    return X_train, X_test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a17249",
   "metadata": {},
   "source": [
    "(d) The next step in our feature engineering process is encoding the categorical variables. We'll define a pipeline step that applies `one-hot encoding` to all `categorical features` in both the training and test datasets. This transformation ensures that categorical values are converted into a numerical format suitable for machine learning models. The function should replace the original categorical columns with their corresponding one-hot encoded values and return the updated train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e945304",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def encode_categorical_values(X_train:pd.DataFrame, X_test:pd.DataFrame) -> Tuple[Annotated[pd.DataFrame, \"X_train_encoded\"],Annotated[pd.DataFrame, \"X_test_encoded\"]]:\n",
    "    \"\"\"\n",
    "    Encodes categorical columns in the training and testing datasets using one-hot encoding.\n",
    "    :param X_train: feature columns of the train set\n",
    "    :param X_test: feature columns of the test set\n",
    "\n",
    "    return train and test sets that have been properly encoded\n",
    "    \"\"\"\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    categorical_columns = X_train.select_dtypes(include=\"object\").columns\n",
    "    encoded_values_train = pd.DataFrame(one_hot_encoder.fit_transform(X_train[categorical_columns]),index=X_train.index,columns=one_hot_encoder.get_feature_names_out())\n",
    "\n",
    "    encoded_values_test = pd.DataFrame(one_hot_encoder.transform(X_test[categorical_columns]),index=X_test.index,columns=one_hot_encoder.get_feature_names_out())\n",
    "    print(encoded_values_test)\n",
    "    X_train.drop(categorical_columns, axis=1, inplace=True)\n",
    "    X_train = pd.concat([X_train, encoded_values_train], axis=1)\n",
    "\n",
    "    X_test.drop(categorical_columns, axis=1, inplace=True)\n",
    "    X_test = pd.concat([X_test, encoded_values_test], axis=1)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad6b2d6",
   "metadata": {},
   "source": [
    "(e) The next feature engineering step in our pipeline is label encoding. Since our target variable, RainTomorrow, is represented as text values ('No' and 'Yes'), we will use `label encoding` to convert these into numerical format. Specifically, the encoder will map 'No' to 0 and 'Yes' to 1, making the target variable suitable for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8aec9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def label_encoding(y_train:pd.Series, y_test:pd.Series) -> Tuple[Annotated[pd.Series, \"y_train_encoded\"], Annotated[pd.Series, \"y_test_encoded\"]]:\n",
    "    \"\"\"\n",
    "    Applies label encoding to the target variable for both training and testing datasets.\n",
    "    :param y_train: target column of the train set\n",
    "    :param y_test: target column of the test set\n",
    "\n",
    "    return train and test target columns that have been properly encoded\n",
    "    \"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = pd.Series(encoder.fit_transform(y_train))\n",
    "    y_test = pd.Series(encoder.transform(y_test))\n",
    "    return y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5ba47",
   "metadata": {},
   "source": [
    "(f) The final feature engineering step in our pipeline is feature scaling. We will use a `MinMaxScaler` to normalize the feature values, scaling them to a range between 0 and 1. This ensures that all features contribute equally to the model's learning process. The scaling will be applied to both the training and test datasets, and the transformed results will be returned as DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d110364",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step \n",
    "def scale_values(X_train:pd.DataFrame,X_test:pd.DataFrame) -> Tuple[Annotated[pd.DataFrame, \"X_train_scaled\"], Annotated[pd.DataFrame, \"X_test_scaled\"]]:\n",
    "    \"\"\"\n",
    "    Scales numerical features to a range between 0 and 1 using MinMax scaling.\n",
    "    :param X_train: feature columns of the train set\n",
    "    :param X_test: feature columns of the test set\n",
    "\n",
    "    return train and test sets that have been properly scaled \n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train),index=X_train.index, columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test),index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2f54c",
   "metadata": {},
   "source": [
    "### 2.2. Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2fe6cf",
   "metadata": {},
   "source": [
    "Now that we've completed all necessary preprocessing steps to create a clean and usable dataset, we're ready to develop and evaluate our machine learning model. Before assembling and executing the full pipeline, we need to define two final steps. Let's try to fit this dataset with a `Logistic Regression` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed7ad29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8c547",
   "metadata": {},
   "source": [
    "(a) The first step is the model_trainer step. This step takes `X_train` (feature set) and `y_train` (corresponding labels) as input. Within the step, a machine learning model is instantiated and trained on the input data. Once training is complete, the fitted model is returned as an artifact, ready for evaluation and inference. Please use the the built-in function `.score()` of the `model` to compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35d544e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def model_trainer(X_train: pd.DataFrame, y_train: pd.Series)-> Tuple[Annotated[ClassifierMixin, \"model\"], Annotated[float, \"in_sample_accuracy\"]]:\n",
    "    \"\"\"\n",
    "    Trains a logistic regression model using the provided training data and computes the in-sample accuracy.\n",
    "    :param X_train: feature columns of the train set\n",
    "    :param y_train: target column of the train set\n",
    "\n",
    "    return a logistic regression model and in-sample accuracy (train accuracy)\n",
    "    \"\"\"\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    in_sample_score = model.score(X_train, y_train)\n",
    "    return model, in_sample_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7679fa",
   "metadata": {},
   "source": [
    "(b) The final step in our pipeline is to evaluate the performance of the trained model. For this, we define the evaluate_model step. It takes three input arguments: the trained model returned by the `model_trainer` step, the preprocessed test features (`X_test`), and the corresponding test labels (`y_test`). Within this step, we calculate the model's accuracy on the test dataset. The resulting accuracy score is then returned as a performance metric. Please use the the built-in function `.score()` of the `model` to compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdc82ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def evaluate_model(model:ClassifierMixin, X_test:pd.DataFrame, y_test:pd.DataFrame) -> Annotated[float, \"accuracy\"]:\n",
    "    \"\"\"\n",
    "    Evaluates the accuracy of a trained model using the testing dataset.\n",
    "    :param model: a trained model\n",
    "    :param X_test: feature columns of the test set\n",
    "    :param y_test: target column of the test set\n",
    "\n",
    "    return out-of-sample accuracy (test accuracy)\n",
    "    \"\"\"\n",
    "    score = model.score(X_test, y_test)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0e75c",
   "metadata": {},
   "source": [
    "### 2.3. Pipeline\n",
    "\n",
    "With all the necessary steps defined, we are now ready to assemble our pipeline. In ZenML, this is done by stacking the individual steps into a single function that represents the pipeline. To create this, we define a new function such as `training_pipeline()`, and annotate it with the `@pipeline` decorator. Please create a pipeline following this procedure:\n",
    "1. Load data\n",
    "2. Split data\n",
    "3. Impute, encode and scale data\n",
    "4. Encode targets\n",
    "5. Train and evaluate the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "811387f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def training_pipeline():\n",
    "    \"\"\"\n",
    "    Executes a full training pipeline on weather data to predict rain tomorrow.\n",
    "    \"\"\"\n",
    "    dataset = loading_data(\"data/weather.csv\")\n",
    "    X_train, X_test,y_train,y_test = split_data(dataset, \"RainTomorrow\")\n",
    "    X_train, X_test = impute_missing_values(X_train, X_test)\n",
    "    X_train, X_test = encode_categorical_values(X_train, X_test)\n",
    "    X_train, X_test = scale_values(X_train, X_test)\n",
    "    y_train, y_test = label_encoding(y_train, y_test)\n",
    "    model, in_sample_score = model_trainer(X_train, y_train)\n",
    "    score = evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a109e9",
   "metadata": {},
   "source": [
    "Let us execute our pipeline by calling our training_pipeline() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b64a610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mInitiating a new run for the pipeline: \u001b[0m\u001b[1;36mtraining_pipeline\u001b[1;35m.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/zenml/integrations/integration.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mIn a future release, the default Python package installer used by ZenML to build container images for your containerized pipelines will change from 'pip' to 'uv'. To maintain current behavior, you can explicitly set \u001b[0m\u001b[1;36mpython_package_installer=PythonPackageInstaller.PIP\u001b[33m in your DockerSettings.\u001b[0m\n",
      "\u001b[1;35mUsing user: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mUsing stack: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  orchestrator: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  artifact_store: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mYou can visualize your pipeline runs in the \u001b[0m\u001b[1;36mZenML Dashboard\u001b[1;35m. In order to try it locally, please run \u001b[0m\u001b[1;36mzenml login --local\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mloading_data\u001b[1;35m has started.\u001b[0m\n",
      "[loading_data] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mloading_data\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.206s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36msplit_data\u001b[1;35m has started.\u001b[0m\n",
      "[split_data] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[split_data] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[split_data] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[split_data] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[split_data] \u001b[33mFailed to extract materializer metadata: Could not convert string 'NoNoYesYesYesNoNoYesYesYesYesNoNoNoYesYesYesYesNoNoNoNoYesNoYesYesYesYesYesYesYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoYesNoNoNoYesNoNoNoNoNoNoNoYesNoYesNoYesYesYesNoYesYesNoNoYesYesNoNoNoYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoYesNoNoNoNoNoYesYesYesYesNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoYesYesYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesNoNoNoNoNoNoNoNoNoYesYesYesNoNoNoNoNoYesYesYesNoNoNoNoNoNoYesYesYesNoYesYesYesNoYesYesYesYesYesNoNoNoNoYesYesYesYesYesYesNoNoNoNoYesYesNoYesYesYesYesYesNoYesNoNoNoNoNoNoNoNoNoNoNoYesNoYesYesYesYesYesYesYesYesYesYesNoYesYesYesYesYesNoNoNoNoYesNoNoNoYesYesNoNoYesNoNoYesNoNoYesYesNoNoYesYesNoYesYesYesYesYesNoNoNoNoNoYesYesYesNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoYesYesNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesNoNoYesNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesYesYesNoNoNoYesNoNoNoNoNoNoNoNoYesYesYesYesNoYesNoNoNoNoNoNoNoNoNoNoNoYesYesYesNoNoNoYesNoNoNoYesNoNoNoNoNoNoNoNoYesYesNoNoYesNoNoNoYesYesNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoYesYesNoNoNoNoNoNoNoNoYesYesYesYesNoNoNoNoNoNoNoNoNoNoNoYesYesYesNoNoYesYesNoNoNoNoYesNoNoYesYesYesYesYesYesNoNoNoNoNoNoNoYesYesNoNoYesYesNoNoNoNoYesYesYesNoNoYesNoYesYesYesYesYesYesYesYesNoNoYesNoYesNoNoNoNoYesYesYesNoNoNoNoNoYesNoYesNoNoNoNoNoNoYesNoYesYesYesNoNoNoNoNoNoNoNoNoNoNoYesYesYesYesNoNoYesNoNoNoYesYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoYesNoYesNoYesNoNoNoNoNoNoYesYesNoYesNoYesNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoYesNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoYesYesNoNoNoYesYesYesYesYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoYesYesNoYesYesYesYesNoYesYesYesNoYesYesYesNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoYesNoNoYesYesNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesYesYesNoNoYesYesNoNoNoNoYesNoYesNoNoNoNoNoYesYesNoNoNoNoNoYesYesNoNoYesNoNoYesYesYesNoNoNoNoNoNoNoNoNoNoNoYesNoYesYesYesNoNoYesYesYesYesNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoYesNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoYesNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoYesNoNoYesNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoYesYesYesYesNoNoNoNoNoYesNoNoYesNoNoNoNoNoNoNoYesNoYesYesNoNoNoNoYesNoNoNoNoYesNoNoNoNoNoNoYesNoNoNoNoNoNoYesYesYesNoNoNoNoNoNoYesYesNoNoNoNoNoYesNoYesNoYesNoYesYesNoNoNoNoNoNoYesYesYesYesYesNoNoYesYesNoNoNoNoYesYesYesYesNoYesNoNoYesYesYesYesYesNoNoNoNoNoNoNoYesYesYesNoYesYesNoNoNoYesNoNoYesYesNoNoYesYesNoYesYesNoYesNoYesYesYesYesYesYesYesNoYesNoNoYesNoNoNoNoYesNoNoNoYesNoNoYesNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoYesNoYesYesYesYesNoNoYesNoNoNoNoNoYesYesYesNoYesYesYesYesNoYesNoNoNoNoYesNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoYesYesYesNoYesNoNoNoNoYesNoNoNoNoYesYesNoYesYesYesYesYesNoNoNoNoYesYesYesNoNoNoYesYesYesYesYesYesYesYesNoNoNoYesNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoYesYesNoYesYesNoNoNoNoNoNoNoYesYesNoNoNoNoNoYesYesYesYesNoYesNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoYesYesYesNoNoNoNoNoYesYesNoNoYesNoNoNoNoNoNoNoNoNoYesYesNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoYesYes' to numeric\u001b[0m\n",
      "[split_data] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[split_data] \u001b[33mFailed to extract materializer metadata: Could not convert string 'NoNoNoNoNoNoNoNoNoYesNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoYesYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoYesNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoYesYesYesYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoYesYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesYesYesNoNoNoNoNoNoNoNoNoYesYesNoYesYesYesYesNoYesNoNoNoNoNoNoNoNoNoYesYesNoYesNoNoNoNoNoYesNoYesNoNoNoNoNoNoNoYesYesYesYesNoNoNoNoNoYesYesNoYesYesYesNoNoNoNoNoNoYesYesNoYesNoNoNoYesNoNoNoNoNoYesYesYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoYesNoNoNoYesNoNoNoNoNoNoNoNoYesNoNoYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoYesNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoYesYesNoNoNoYesNoYesNoNoNoNoNoYesYesYesYesYesYesNoNoNoNoNoYesYesYesNoNoNoNoNoYesNoYesNoYesNoNoYesYesYesYesYesYesNoYesNoNoNoNoNoNoYesNoNoYesYesNoYesYesNoYesNoNoNoNoNoNoYesYesYesYesNoYesYesNoNoYesNoNoNoNoNoNoNoNoYesYesYesYesNoNoNoNoNoYesYesNoNoYesYesNoNoNoNoNoNoNoNoYesNoNoNoYesYesNoYesYesYesYesNoYesNoNoNoYesYesYesNoYesYesYesNoNoNoNoYesYesNoNoYesYesNoNoNoNoNoYesYesNoNoNoNoNoNoNoYesNoNoYesNoNoNoYesYesYesNoYesYesNoNoYesYesNoNoNoNoYesYesYesNoNoNoNoNoYesYesNoNoNoYesNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesYesNoNoNoNoNoNoNoYesYesYesNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoYesYesYesNoNoYesNoNoNoYesNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoYesNoNoNoNoNoYesYesNoNoNoYesYesYesYesNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoYesNoNoNoNo' to numeric\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36msplit_data\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.365s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mimpute_missing_values\u001b[1;35m has started.\u001b[0m\n",
      "[impute_missing_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[impute_missing_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[impute_missing_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[impute_missing_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mimpute_missing_values\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.272s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mlabel_encoding\u001b[1;35m has started.\u001b[0m\n",
      "[label_encoding] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[label_encoding] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[label_encoding] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[label_encoding] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mlabel_encoding\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.222s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mencode_categorical_values\u001b[1;35m has started.\u001b[0m\n",
      "[encode_categorical_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[encode_categorical_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[encode_categorical_values]             WindGustDir_E  WindGustDir_ENE  WindGustDir_ESE  WindGustDir_N  \\\n",
      "Date                                                                         \n",
      "2014-11-11            0.0              0.0              0.0            0.0   \n",
      "2014-11-12            0.0              0.0              0.0            0.0   \n",
      "2014-11-13            0.0              0.0              0.0            0.0   \n",
      "2014-11-14            1.0              0.0              0.0            0.0   \n",
      "2014-11-15            0.0              0.0              0.0            0.0   \n",
      "...                   ...              ...              ...            ...   \n",
      "2017-06-21            0.0              0.0              0.0            0.0   \n",
      "2017-06-22            0.0              0.0              0.0            0.0   \n",
      "2017-06-23            0.0              0.0              0.0            0.0   \n",
      "2017-06-24            0.0              0.0              0.0            0.0   \n",
      "2017-06-25            1.0              0.0              0.0            0.0   \n",
      "\n",
      "            WindGustDir_NE  WindGustDir_NNE  WindGustDir_NNW  WindGustDir_NW  \\\n",
      "Date                                                                           \n",
      "2014-11-11             0.0              0.0              0.0             0.0   \n",
      "2014-11-12             0.0              0.0              0.0             0.0   \n",
      "2014-11-13             0.0              0.0              0.0             0.0   \n",
      "2014-11-14             0.0              0.0              0.0             0.0   \n",
      "2014-11-15             0.0              0.0              0.0             0.0   \n",
      "...                    ...              ...              ...             ...   \n",
      "2017-06-21             0.0              0.0              0.0             1.0   \n",
      "2017-06-22             0.0              0.0              0.0             0.0   \n",
      "2017-06-23             0.0              0.0              0.0             0.0   \n",
      "2017-06-24             0.0              0.0              0.0             0.0   \n",
      "2017-06-25             0.0              0.0              0.0             0.0   \n",
      "\n",
      "            WindGustDir_S  WindGustDir_SE  ...  WindDir3pm_NNW  WindDir3pm_NW  \\\n",
      "Date                                       ...                                  \n",
      "2014-11-11            0.0             0.0  ...             0.0            0.0   \n",
      "2014-11-12            0.0             0.0  ...             0.0            0.0   \n",
      "2014-11-13            0.0             0.0  ...             0.0            0.0   \n",
      "2014-11-14            0.0             0.0  ...             0.0            0.0   \n",
      "2014-11-15            0.0             0.0  ...             0.0            0.0   \n",
      "...                   ...             ...  ...             ...            ...   \n",
      "2017-06-21            0.0             0.0  ...             1.0            0.0   \n",
      "2017-06-22            0.0             0.0  ...             0.0            0.0   \n",
      "2017-06-23            0.0             1.0  ...             0.0            0.0   \n",
      "2017-06-24            0.0             1.0  ...             0.0            0.0   \n",
      "2017-06-25            0.0             0.0  ...             0.0            0.0   \n",
      "\n",
      "            WindDir3pm_S  WindDir3pm_SE  WindDir3pm_SSE  WindDir3pm_SSW  \\\n",
      "Date                                                                      \n",
      "2014-11-11           0.0            0.0             0.0             0.0   \n",
      "2014-11-12           0.0            0.0             0.0             1.0   \n",
      "2014-11-13           0.0            0.0             0.0             1.0   \n",
      "2014-11-14           0.0            0.0             0.0             0.0   \n",
      "2014-11-15           0.0            0.0             0.0             0.0   \n",
      "...                  ...            ...             ...             ...   \n",
      "2017-06-21           0.0            0.0             0.0             0.0   \n",
      "2017-06-22           0.0            0.0             0.0             0.0   \n",
      "2017-06-23           0.0            0.0             0.0             0.0   \n",
      "2017-06-24           0.0            0.0             0.0             0.0   \n",
      "2017-06-25           0.0            1.0             0.0             0.0   \n",
      "\n",
      "            WindDir3pm_SW  WindDir3pm_W  WindDir3pm_WNW  WindDir3pm_WSW  \n",
      "Date                                                                     \n",
      "2014-11-11            0.0           0.0             0.0             1.0  \n",
      "2014-11-12            0.0           0.0             0.0             0.0  \n",
      "2014-11-13            0.0           0.0             0.0             0.0  \n",
      "2014-11-14            1.0           0.0             0.0             0.0  \n",
      "2014-11-15            1.0           0.0             0.0             0.0  \n",
      "...                   ...           ...             ...             ...  \n",
      "2017-06-21            0.0           0.0             0.0             0.0  \n",
      "2017-06-22            1.0           0.0             0.0             0.0  \n",
      "2017-06-23            0.0           0.0             0.0             0.0  \n",
      "2017-06-24            0.0           0.0             0.0             0.0  \n",
      "2017-06-25            0.0           0.0             0.0             0.0  \n",
      "\n",
      "[958 rows x 48 columns]\n",
      "[encode_categorical_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[encode_categorical_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mencode_categorical_values\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.381s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mscale_values\u001b[1;35m has started.\u001b[0m\n",
      "[scale_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[scale_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[scale_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[scale_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mscale_values\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.408s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_trainer\u001b[1;35m has started.\u001b[0m\n",
      "[model_trainer] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[model_trainer] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[model_trainer] /home/vscode/.local/lib/python3.10/site-packages/zenml/integrations/pandas/materializers/pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_trainer\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.503s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mevaluate_model\u001b[1;35m has started.\u001b[0m\n",
      "[evaluate_model] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[evaluate_model] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[evaluate_model] /home/vscode/.local/lib/python3.10/site-packages/zenml/integrations/pandas/materializers/pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mevaluate_model\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.278s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mPipeline run has finished in \u001b[0m\u001b[1;36m5.557s\u001b[1;35m.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineRunResponse(body=PipelineRunResponseBody(created=datetime.datetime(2025, 5, 29, 22, 47, 47, 700136), updated=datetime.datetime(2025, 5, 29, 22, 47, 53, 285850), user_id=UUID('e4b90bb5-714c-4daf-aed3-77b52adbf0dd'), project_id=UUID('36a9770f-837c-49ea-91e4-fea5baf0f818'), status=<ExecutionStatus.COMPLETED: 'completed'>, stack=StackResponse(body=StackResponseBody(created=datetime.datetime(2025, 5, 29, 19, 45, 26, 1397), updated=datetime.datetime(2025, 5, 29, 19, 45, 26, 1414), user_id=None), metadata=None, resources=None, id=UUID('7fed847a-0444-45d1-bade-678d2d045f31'), permission_denied=False, name='default'), pipeline=PipelineResponse(body=PipelineResponseBody(created=datetime.datetime(2025, 5, 29, 20, 12, 52, 587050), updated=datetime.datetime(2025, 5, 29, 20, 12, 52, 587067), user_id=UUID('e4b90bb5-714c-4daf-aed3-77b52adbf0dd'), project_id=UUID('36a9770f-837c-49ea-91e4-fea5baf0f818')), metadata=None, resources=None, id=UUID('c6f6c1f4-38a8-48d7-af59-98c365cc0d06'), permission_denied=False, name='training_pipeline'), build=None, schedule=None, code_reference=None, deployment_id=UUID('ceaafc50-ba64-4893-b95b-1f99ef8be52f'), trigger_execution=None, model_version_id=None), metadata=PipelineRunResponseMetadata(run_metadata={}, config=PipelineConfiguration(enable_cache=None, enable_artifact_metadata=None, enable_artifact_visualization=None, enable_step_logs=None, enable_pipeline_logs=None, settings={}, tags=None, extra={}, failure_hook_source=None, success_hook_source=None, model=None, parameters=None, retry=None, substitutions={'date': '2025_05_29', 'time': '22_47_47_696330'}, name='training_pipeline'), start_time=datetime.datetime(2025, 5, 29, 22, 47, 47, 696330), end_time=datetime.datetime(2025, 5, 29, 22, 47, 53, 87884), client_environment={'environment': 'vscode_remote_container', 'os': 'linux', 'linux_distro': 'debian', 'linux_distro_like': '', 'linux_distro_version': '11', 'python_version': '3.10.12'}, orchestrator_environment={'environment': 'vscode_remote_container', 'os': 'linux', 'linux_distro': 'debian', 'linux_distro_like': '', 'linux_distro_version': '11', 'python_version': '3.10.12'}, orchestrator_run_id='d4dca377-99fa-4fdb-85f6-c7d8c444dc83', code_path=None, template_id=None, is_templatable=False), resources=PipelineRunResponseResources(user=UserResponse(body=UserResponseBody(created=datetime.datetime(2025, 5, 29, 19, 45, 26, 223991), updated=datetime.datetime(2025, 5, 29, 19, 45, 26, 224012), active=True, activation_token=None, full_name='', email_opted_in=None, is_service_account=False, is_admin=True, default_project_id=None), metadata=None, resources=None, id=UUID('e4b90bb5-714c-4daf-aed3-77b52adbf0dd'), permission_denied=False, name='default'), model_version=None, tags=[], logs=LogsResponse(body=LogsResponseBody(created=datetime.datetime(2025, 5, 29, 22, 47, 47, 704997), updated=datetime.datetime(2025, 5, 29, 22, 47, 47, 705012), uri='/home/vscode/.config/zenml/local_stores/ced76857-a059-48c0-89cc-5d98c1af0040/pipeline_runs/logs/e04b6023-cf5c-4b47-ba49-bde707ff99f1.log'), metadata=None, resources=None, id=UUID('599048f6-365f-4dfe-b695-ffba65b3dfee'), permission_denied=False)), id=UUID('97370b11-8792-497e-a552-5bbd1f62563f'), permission_denied=False, name='training_pipeline-2025_05_29-22_47_47_696330')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell\n",
    "training_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd05947",
   "metadata": {},
   "source": [
    "### 2.4. Evaluating performance by loading artifacts\n",
    "\n",
    "Let's now retrieve the artifacts from the pipeline for different purposes. We first create a client instance to interact with the ZenML backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f73e7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "from zenml.client import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345f37e3",
   "metadata": {},
   "source": [
    "(a) Retrieve the test accuracy artifact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f30579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_artifact = client.get_artifact_version(\"accuracy\")\n",
    "# no need to modify this\n",
    "test_acc_re = acc_artifact.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddd13263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "# if your pipeline is correct, you should obtain accuracy >= 90%\n",
    "assert test_acc_re >= 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba66a5e",
   "metadata": {},
   "source": [
    "(b) You can also retrieve the trained model by loading the artifact from the `model_trainer` step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a9ed796",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact = client.get_artifact_version(\"model\")\n",
    "# no need to modify this\n",
    "model_re = model_artifact.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429b8ca",
   "metadata": {},
   "source": [
    "(c) Please retrieve:\n",
    "- `X_test` artifact from `scale_values` step\n",
    "- `y_test` artifact from `label_encoding` step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5322e3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/zenml/integrations/pandas/materializers/pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n"
     ]
    }
   ],
   "source": [
    "X_test_artifact = client.get_artifact_version(\"X_test_scaled\")\n",
    "y_test_artifact = client.get_artifact_version(\"y_test_encoded\")\n",
    "# no need to modify these, please ignore the warnings\n",
    "X_test_re = X_test_artifact.load()\n",
    "y_test_re = y_test_artifact.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a1471",
   "metadata": {},
   "source": [
    "(d) Use the retrieved model (`model_re`) to compute the accuracy using the retrieved data (`X_test_re`, `y_test_re`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5427f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model_re.score(X_test_re, y_test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06f9e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell, the retrieved accuracy and the newly computed one should be equal\n",
    "# the reason is, we are using the same trained model to compute the accuracy\n",
    "assert test_acc_re == test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a13d1",
   "metadata": {},
   "source": [
    "## 3. Evaluating Model Robustness via Artifact Resuse in ZenML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9236e6c",
   "metadata": {},
   "source": [
    "In this exercise, you will build upon the pipeline you created in Exercise 2 by adding a new step that **perturbs the test data with Gaussian (white) noise** before evaluating the omdel's performance. Instead of testing on the original test set (`X_test`), you will:\n",
    "\n",
    "- Apply random noise to the numerical features of the test data, simulating real-world data imperfections or sensor noise.\n",
    "- Evaluate the trained model on this noisy test set to observe how its performance change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c852403",
   "metadata": {},
   "source": [
    "### Why perturbing the test data?\n",
    "\n",
    "Machine learning models often perform well on clean, well-prepared data but can be sensitive to small perturbations or noise in inputs. This sensitivity reflects a model’s robustness or generalization ability when facing slightly altered or imperfect data. For example, Gaussian noise (random variation with zero mean) can simulate measurement errors or environmental variations that frequently occur in real-world scenarios. \n",
    "\n",
    "More importantly, this sensitivity is related to a broader challenge known as adversarial robustness: small, carefully crafted perturbations, called [adversarial attacks](https://medium.com/@yashgaherwar2002/adversarial-machine-learning-attacks-preventions-640c5ffc2404), which can drastically change a model’s predictions despite being imperceptible to humans. While Gaussian  noise is random and unstructured, adversarial perturbations are deliberate and exploit model vulnerabilities. Studying how models perform under random noise is a first step toward understanding and improving their robustness against such adversarial manipulations and other real-world data shifts.\n",
    "\n",
    "To briefly illustrate the concept of adversarial examples, consider the following image:\n",
    "\n",
    "\n",
    "![panda](https://miro.medium.com/v2/resize:fit:720/format:webp/0*iGzOt4oCR74nMNgu)\n",
    "\n",
    "In this example, an image of a panda is correctly classified by a machine learning model. However, by adding subtle perturbations (undetected by human eyes), the model misclassifies the image as a \"gibbon\" with high confidence. This phenomenon demonstrates the vulnerability of machine learning models to small, intentional changes in input data, which highlights the importance of evaluating model robustness. If you are interested, here is a good [resource](https://arxiv.org/pdf/1412.6572) to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea07fd2",
   "metadata": {},
   "source": [
    "(a) Create a ZenML step to perturb the input data `X` with Gaussian noise using [numpy.random.normal](https://numpy.org/doc/2.1/reference/random/generated/numpy.random.normal.html).\n",
    "- Hint 1: extract numeric columns using `X.select_dtypes()` to get numerical columns only. Please refer back to assignment 1 and tutorial 2.\n",
    "- Hint 2: create `noise` variable with the following parameters:\n",
    "    - `loc=0.0`\n",
    "    - `scale=0.5`\n",
    "    - `size=X[extracted_numerical_columns].shape`\n",
    "- Hint 3: Add this noise into X using `X.loc[:, extracted_numerical_columns] += noise`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7f899cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "@step\n",
    "def perturb_data(X: pd.DataFrame) -> Annotated[pd.DataFrame, \"perturbed_x\"] :\n",
    "    \"\"\"Applies Gaussian noise to numerical features of X.\"\"\"\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    # Add noise only to numeric columns\n",
    "    noise = np.random.normal(0, 0.5, size=X[numeric_cols].shape)\n",
    "    X.loc[:, numeric_cols] += noise\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d85e8",
   "metadata": {},
   "source": [
    "(b) Create a new pipeline with the following steps:\n",
    "1. Load data\n",
    "2. Split data\n",
    "3. Impute, encode and scale data\n",
    "4. Encode targets\n",
    "5. Train the model using `X_train`, `y_train`\n",
    "6. Perturb `X_test`\n",
    "7. Evaluate the model using `perturbed_X_test`, `y_test`\n",
    "\n",
    "Hint: re-use the steps from exercise 2 and combine it with 3(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b777d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def robustness_evaluation_pipeline():\n",
    "    dataset = loading_data(\"data/weather.csv\")\n",
    "    X_train, X_test, y_train, y_test = split_data(dataset, \"RainTomorrow\")\n",
    "    X_train, X_test = impute_missing_values(X_train, X_test)\n",
    "    X_train, X_test = encode_categorical_values(X_train, X_test)\n",
    "    X_train, X_test = scale_values(X_train, X_test)\n",
    "    y_train, y_test = label_encoding(y_train, y_test)\n",
    "    perturbed_X_test = perturb_data(X_test)\n",
    "    model, in_sample_score = model_trainer(X_train, y_train)\n",
    "    score = evaluate_model(model, perturbed_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bcdaece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mInitiating a new run for the pipeline: \u001b[0m\u001b[1;36mrobustness_evaluation_pipeline\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mUsing user: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mUsing stack: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  orchestrator: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  artifact_store: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mYou can visualize your pipeline runs in the \u001b[0m\u001b[1;36mZenML Dashboard\u001b[1;35m. In order to try it locally, please run \u001b[0m\u001b[1;36mzenml login --local\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mloading_data\u001b[1;35m has started.\u001b[0m\n",
      "[loading_data] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mloading_data\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.154s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36msplit_data\u001b[1;35m has started.\u001b[0m\n",
      "[split_data] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[split_data] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[split_data] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[split_data] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[split_data] \u001b[33mFailed to extract materializer metadata: Could not convert string 'NoNoYesYesYesNoNoYesYesYesYesNoNoNoYesYesYesYesNoNoNoNoYesNoYesYesYesYesYesYesYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoYesNoNoNoYesNoNoNoNoNoNoNoYesNoYesNoYesYesYesNoYesYesNoNoYesYesNoNoNoYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoYesNoNoNoNoNoYesYesYesYesNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoYesYesYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesNoNoNoNoNoNoNoNoNoYesYesYesNoNoNoNoNoYesYesYesNoNoNoNoNoNoYesYesYesNoYesYesYesNoYesYesYesYesYesNoNoNoNoYesYesYesYesYesYesNoNoNoNoYesYesNoYesYesYesYesYesNoYesNoNoNoNoNoNoNoNoNoNoNoYesNoYesYesYesYesYesYesYesYesYesYesNoYesYesYesYesYesNoNoNoNoYesNoNoNoYesYesNoNoYesNoNoYesNoNoYesYesNoNoYesYesNoYesYesYesYesYesNoNoNoNoNoYesYesYesNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoYesYesNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesNoNoYesNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesYesYesNoNoNoYesNoNoNoNoNoNoNoNoYesYesYesYesNoYesNoNoNoNoNoNoNoNoNoNoNoYesYesYesNoNoNoYesNoNoNoYesNoNoNoNoNoNoNoNoYesYesNoNoYesNoNoNoYesYesNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoYesYesNoNoNoNoNoNoNoNoYesYesYesYesNoNoNoNoNoNoNoNoNoNoNoYesYesYesNoNoYesYesNoNoNoNoYesNoNoYesYesYesYesYesYesNoNoNoNoNoNoNoYesYesNoNoYesYesNoNoNoNoYesYesYesNoNoYesNoYesYesYesYesYesYesYesYesNoNoYesNoYesNoNoNoNoYesYesYesNoNoNoNoNoYesNoYesNoNoNoNoNoNoYesNoYesYesYesNoNoNoNoNoNoNoNoNoNoNoYesYesYesYesNoNoYesNoNoNoYesYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoYesNoYesNoYesNoNoNoNoNoNoYesYesNoYesNoYesNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoYesNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoYesYesNoNoNoYesYesYesYesYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoYesYesNoYesYesYesYesNoYesYesYesNoYesYesYesNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoYesNoNoYesYesNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesYesYesNoNoYesYesNoNoNoNoYesNoYesNoNoNoNoNoYesYesNoNoNoNoNoYesYesNoNoYesNoNoYesYesYesNoNoNoNoNoNoNoNoNoNoNoYesNoYesYesYesNoNoYesYesYesYesNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoYesNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoYesNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoYesNoNoYesNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoYesYesYesYesNoNoNoNoNoYesNoNoYesNoNoNoNoNoNoNoYesNoYesYesNoNoNoNoYesNoNoNoNoYesNoNoNoNoNoNoYesNoNoNoNoNoNoYesYesYesNoNoNoNoNoNoYesYesNoNoNoNoNoYesNoYesNoYesNoYesYesNoNoNoNoNoNoYesYesYesYesYesNoNoYesYesNoNoNoNoYesYesYesYesNoYesNoNoYesYesYesYesYesNoNoNoNoNoNoNoYesYesYesNoYesYesNoNoNoYesNoNoYesYesNoNoYesYesNoYesYesNoYesNoYesYesYesYesYesYesYesNoYesNoNoYesNoNoNoNoYesNoNoNoYesNoNoYesNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoYesNoYesYesYesYesNoNoYesNoNoNoNoNoYesYesYesNoYesYesYesYesNoYesNoNoNoNoYesNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoYesYesYesNoYesNoNoNoNoYesNoNoNoNoYesYesNoYesYesYesYesYesNoNoNoNoYesYesYesNoNoNoYesYesYesYesYesYesYesYesNoNoNoYesNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoYesYesNoYesYesNoNoNoNoNoNoNoYesYesNoNoNoNoNoYesYesYesYesNoYesNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoYesYesYesNoNoNoNoNoYesYesNoNoYesNoNoNoNoNoNoNoNoNoYesYesNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoYesYes' to numeric\u001b[0m\n",
      "[split_data] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[split_data] \u001b[33mFailed to extract materializer metadata: Could not convert string 'NoNoNoNoNoNoNoNoNoYesNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoYesYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoYesNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoYesYesYesYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoYesYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesYesYesNoNoNoNoNoNoNoNoNoYesYesNoYesYesYesYesNoYesNoNoNoNoNoNoNoNoNoYesYesNoYesNoNoNoNoNoYesNoYesNoNoNoNoNoNoNoYesYesYesYesNoNoNoNoNoYesYesNoYesYesYesNoNoNoNoNoNoYesYesNoYesNoNoNoYesNoNoNoNoNoYesYesYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoYesNoNoNoYesNoNoNoNoNoNoNoNoYesNoNoYesNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoYesNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoYesYesNoNoNoYesNoYesNoNoNoNoNoYesYesYesYesYesYesNoNoNoNoNoYesYesYesNoNoNoNoNoYesNoYesNoYesNoNoYesYesYesYesYesYesNoYesNoNoNoNoNoNoYesNoNoYesYesNoYesYesNoYesNoNoNoNoNoNoYesYesYesYesNoYesYesNoNoYesNoNoNoNoNoNoNoNoYesYesYesYesNoNoNoNoNoYesYesNoNoYesYesNoNoNoNoNoNoNoNoYesNoNoNoYesYesNoYesYesYesYesNoYesNoNoNoYesYesYesNoYesYesYesNoNoNoNoYesYesNoNoYesYesNoNoNoNoNoYesYesNoNoNoNoNoNoNoYesNoNoYesNoNoNoYesYesYesNoYesYesNoNoYesYesNoNoNoNoYesYesYesNoNoNoNoNoYesYesNoNoNoYesNoNoNoNoNoNoNoNoYesYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesYesYesYesNoNoNoNoNoNoNoYesYesYesNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoNoNoNoYesYesYesNoNoYesNoNoNoYesNoNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoYesNoNoNoNoNoYesYesNoNoNoYesYesYesYesNoNoYesNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoYesNoNoNoNoNoNoNoYesNoNoNoNo' to numeric\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36msplit_data\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.389s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mimpute_missing_values\u001b[1;35m has started.\u001b[0m\n",
      "[impute_missing_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[impute_missing_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[impute_missing_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[impute_missing_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mimpute_missing_values\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.278s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mlabel_encoding\u001b[1;35m has started.\u001b[0m\n",
      "[label_encoding] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[label_encoding] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[label_encoding] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[label_encoding] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mlabel_encoding\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.224s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mencode_categorical_values\u001b[1;35m has started.\u001b[0m\n",
      "[encode_categorical_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[encode_categorical_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[encode_categorical_values]             WindGustDir_E  WindGustDir_ENE  WindGustDir_ESE  WindGustDir_N  \\\n",
      "Date                                                                         \n",
      "2014-11-11            0.0              0.0              0.0            0.0   \n",
      "2014-11-12            0.0              0.0              0.0            0.0   \n",
      "2014-11-13            0.0              0.0              0.0            0.0   \n",
      "2014-11-14            1.0              0.0              0.0            0.0   \n",
      "2014-11-15            0.0              0.0              0.0            0.0   \n",
      "...                   ...              ...              ...            ...   \n",
      "2017-06-21            0.0              0.0              0.0            0.0   \n",
      "2017-06-22            0.0              0.0              0.0            0.0   \n",
      "2017-06-23            0.0              0.0              0.0            0.0   \n",
      "2017-06-24            0.0              0.0              0.0            0.0   \n",
      "2017-06-25            1.0              0.0              0.0            0.0   \n",
      "\n",
      "            WindGustDir_NE  WindGustDir_NNE  WindGustDir_NNW  WindGustDir_NW  \\\n",
      "Date                                                                           \n",
      "2014-11-11             0.0              0.0              0.0             0.0   \n",
      "2014-11-12             0.0              0.0              0.0             0.0   \n",
      "2014-11-13             0.0              0.0              0.0             0.0   \n",
      "2014-11-14             0.0              0.0              0.0             0.0   \n",
      "2014-11-15             0.0              0.0              0.0             0.0   \n",
      "...                    ...              ...              ...             ...   \n",
      "2017-06-21             0.0              0.0              0.0             1.0   \n",
      "2017-06-22             0.0              0.0              0.0             0.0   \n",
      "2017-06-23             0.0              0.0              0.0             0.0   \n",
      "2017-06-24             0.0              0.0              0.0             0.0   \n",
      "2017-06-25             0.0              0.0              0.0             0.0   \n",
      "\n",
      "            WindGustDir_S  WindGustDir_SE  ...  WindDir3pm_NNW  WindDir3pm_NW  \\\n",
      "Date                                       ...                                  \n",
      "2014-11-11            0.0             0.0  ...             0.0            0.0   \n",
      "2014-11-12            0.0             0.0  ...             0.0            0.0   \n",
      "2014-11-13            0.0             0.0  ...             0.0            0.0   \n",
      "2014-11-14            0.0             0.0  ...             0.0            0.0   \n",
      "2014-11-15            0.0             0.0  ...             0.0            0.0   \n",
      "...                   ...             ...  ...             ...            ...   \n",
      "2017-06-21            0.0             0.0  ...             1.0            0.0   \n",
      "2017-06-22            0.0             0.0  ...             0.0            0.0   \n",
      "2017-06-23            0.0             1.0  ...             0.0            0.0   \n",
      "2017-06-24            0.0             1.0  ...             0.0            0.0   \n",
      "2017-06-25            0.0             0.0  ...             0.0            0.0   \n",
      "\n",
      "            WindDir3pm_S  WindDir3pm_SE  WindDir3pm_SSE  WindDir3pm_SSW  \\\n",
      "Date                                                                      \n",
      "2014-11-11           0.0            0.0             0.0             0.0   \n",
      "2014-11-12           0.0            0.0             0.0             1.0   \n",
      "2014-11-13           0.0            0.0             0.0             1.0   \n",
      "2014-11-14           0.0            0.0             0.0             0.0   \n",
      "2014-11-15           0.0            0.0             0.0             0.0   \n",
      "...                  ...            ...             ...             ...   \n",
      "2017-06-21           0.0            0.0             0.0             0.0   \n",
      "2017-06-22           0.0            0.0             0.0             0.0   \n",
      "2017-06-23           0.0            0.0             0.0             0.0   \n",
      "2017-06-24           0.0            0.0             0.0             0.0   \n",
      "2017-06-25           0.0            1.0             0.0             0.0   \n",
      "\n",
      "            WindDir3pm_SW  WindDir3pm_W  WindDir3pm_WNW  WindDir3pm_WSW  \n",
      "Date                                                                     \n",
      "2014-11-11            0.0           0.0             0.0             1.0  \n",
      "2014-11-12            0.0           0.0             0.0             0.0  \n",
      "2014-11-13            0.0           0.0             0.0             0.0  \n",
      "2014-11-14            1.0           0.0             0.0             0.0  \n",
      "2014-11-15            1.0           0.0             0.0             0.0  \n",
      "...                   ...           ...             ...             ...  \n",
      "2017-06-21            0.0           0.0             0.0             0.0  \n",
      "2017-06-22            1.0           0.0             0.0             0.0  \n",
      "2017-06-23            0.0           0.0             0.0             0.0  \n",
      "2017-06-24            0.0           0.0             0.0             0.0  \n",
      "2017-06-25            0.0           0.0             0.0             0.0  \n",
      "\n",
      "[958 rows x 48 columns]\n",
      "[encode_categorical_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[encode_categorical_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mencode_categorical_values\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.375s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mscale_values\u001b[1;35m has started.\u001b[0m\n",
      "[scale_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[scale_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[scale_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[scale_values] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mscale_values\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.404s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_trainer\u001b[1;35m has started.\u001b[0m\n",
      "[model_trainer] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[model_trainer] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[model_trainer] /home/vscode/.local/lib/python3.10/site-packages/zenml/integrations/pandas/materializers/pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_trainer\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.555s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mperturb_data\u001b[1;35m has started.\u001b[0m\n",
      "[perturb_data] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[perturb_data] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mperturb_data\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.226s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mevaluate_model\u001b[1;35m has started.\u001b[0m\n",
      "[evaluate_model] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n",
      "[evaluate_model] \u001b[33mBy default, the \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[1;36m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[1;36mpyarrow\u001b[33m by running '\u001b[0m\u001b[1;36mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[1;36mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[1;36m.parquet\u001b[33m file instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[evaluate_model] /home/vscode/.local/lib/python3.10/site-packages/zenml/integrations/pandas/materializers/pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mevaluate_model\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.241s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mPipeline run has finished in \u001b[0m\u001b[1;36m6.164s\u001b[1;35m.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineRunResponse(body=PipelineRunResponseBody(created=datetime.datetime(2025, 5, 29, 22, 47, 54, 788994), updated=datetime.datetime(2025, 5, 29, 22, 48, 0, 959403), user_id=UUID('e4b90bb5-714c-4daf-aed3-77b52adbf0dd'), project_id=UUID('36a9770f-837c-49ea-91e4-fea5baf0f818'), status=<ExecutionStatus.COMPLETED: 'completed'>, stack=StackResponse(body=StackResponseBody(created=datetime.datetime(2025, 5, 29, 19, 45, 26, 1397), updated=datetime.datetime(2025, 5, 29, 19, 45, 26, 1414), user_id=None), metadata=None, resources=None, id=UUID('7fed847a-0444-45d1-bade-678d2d045f31'), permission_denied=False, name='default'), pipeline=PipelineResponse(body=PipelineResponseBody(created=datetime.datetime(2025, 5, 29, 22, 21, 54, 301866), updated=datetime.datetime(2025, 5, 29, 22, 21, 54, 301883), user_id=UUID('e4b90bb5-714c-4daf-aed3-77b52adbf0dd'), project_id=UUID('36a9770f-837c-49ea-91e4-fea5baf0f818')), metadata=None, resources=None, id=UUID('3444efd0-6507-4f7b-aa1a-a887086de239'), permission_denied=False, name='robustness_evaluation_pipeline'), build=None, schedule=None, code_reference=None, deployment_id=UUID('504f7b64-9a79-4a15-a621-a4cf35008e11'), trigger_execution=None, model_version_id=None), metadata=PipelineRunResponseMetadata(run_metadata={}, config=PipelineConfiguration(enable_cache=None, enable_artifact_metadata=None, enable_artifact_visualization=None, enable_step_logs=None, enable_pipeline_logs=None, settings={}, tags=None, extra={}, failure_hook_source=None, success_hook_source=None, model=None, parameters=None, retry=None, substitutions={'date': '2025_05_29', 'time': '22_47_54_787347'}, name='robustness_evaluation_pipeline'), start_time=datetime.datetime(2025, 5, 29, 22, 47, 54, 787347), end_time=datetime.datetime(2025, 5, 29, 22, 48, 0, 804836), client_environment={'environment': 'vscode_remote_container', 'os': 'linux', 'linux_distro': 'debian', 'linux_distro_like': '', 'linux_distro_version': '11', 'python_version': '3.10.12'}, orchestrator_environment={'environment': 'vscode_remote_container', 'os': 'linux', 'linux_distro': 'debian', 'linux_distro_like': '', 'linux_distro_version': '11', 'python_version': '3.10.12'}, orchestrator_run_id='38488365-fe59-42d0-8ec9-b82bfbb428aa', code_path=None, template_id=None, is_templatable=False), resources=PipelineRunResponseResources(user=UserResponse(body=UserResponseBody(created=datetime.datetime(2025, 5, 29, 19, 45, 26, 223991), updated=datetime.datetime(2025, 5, 29, 19, 45, 26, 224012), active=True, activation_token=None, full_name='', email_opted_in=None, is_service_account=False, is_admin=True, default_project_id=None), metadata=None, resources=None, id=UUID('e4b90bb5-714c-4daf-aed3-77b52adbf0dd'), permission_denied=False, name='default'), model_version=None, tags=[], logs=LogsResponse(body=LogsResponseBody(created=datetime.datetime(2025, 5, 29, 22, 47, 54, 790199), updated=datetime.datetime(2025, 5, 29, 22, 47, 54, 790211), uri='/home/vscode/.config/zenml/local_stores/ced76857-a059-48c0-89cc-5d98c1af0040/pipeline_runs/logs/53754612-3eab-4258-8d62-07d6c1b6f8f4.log'), metadata=None, resources=None, id=UUID('657798c9-9312-4f32-80ba-92930d1d6ef9'), permission_denied=False)), id=UUID('dcc7ee0f-802e-499e-b0cb-cf7587f2c5b9'), permission_denied=False, name='robustness_evaluation_pipeline-2025_05_29-22_47_54_787347')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robustness_evaluation_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66207670",
   "metadata": {},
   "source": [
    "(c) Let's use the client from `exercise 2.4` to load the `accuracy` artifact again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77c9ffd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7390396659707724"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_acc_artifact = client.get_artifact_version(\"accuracy\")\n",
    "# no need to modify this\n",
    "perturbed_test_acc = perturbed_acc_artifact.load()\n",
    "perturbed_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b89923",
   "metadata": {},
   "source": [
    "You would notice that the performance is now worse than that of the original test set. If you increase the `scale` parameter of `numpy.random.normal`, you will notice that the performance drops further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5830a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "assert perturbed_test_acc < test_acc_re"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
